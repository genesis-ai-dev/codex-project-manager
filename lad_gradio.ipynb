{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: /usr/local/bin/pip: bad interpreter: /usr/local/opt/python@3.10/bin/python3.10: no such file or directory\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "codex-python-types 0.0.3 requires typing-extensions==4.9.0, but you have typing-extensions 4.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install gradio pydantic nltk sacrebleu numpy -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from pydantic import BaseModel\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sacrebleu import corpus_bleu\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "class ScoredLine(BaseModel):\n",
    "    content: str\n",
    "    scores: list[float]\n",
    "    normalized_score: float\n",
    "\n",
    "class Corpus(BaseModel):\n",
    "    text: str\n",
    "\n",
    "    @property\n",
    "    def lines(self):\n",
    "        return [ScoredLine(content=line, scores=[], normalized_score=0.0) for line in self.text.split('\\n')]\n",
    "\n",
    "    def calculate_scores(self):\n",
    "        for line in self.lines:\n",
    "            line.scores = [\n",
    "                self.relative_length(line.content),\n",
    "                self.calculate_bleu_score(line.content),\n",
    "                # Add more scoring methods here\n",
    "            ]\n",
    "            line.normalized_score = np.mean(line.scores)\n",
    "\n",
    "    def relative_length(self, line):\n",
    "        # Calculate relative length score\n",
    "        return len(line) / len(self.text)\n",
    "\n",
    "    def calculate_bleu_score(self, line):\n",
    "        # Calculate BLEU score for the line\n",
    "        reference = [self.text.split()]\n",
    "        hypothesis = line.split()\n",
    "        smoothie = SmoothingFunction().method4\n",
    "        return sentence_bleu(reference, hypothesis, smoothing_function=smoothie)\n",
    "\n",
    "class CrossLinguisticComparison(BaseModel):\n",
    "    corpus1: Corpus\n",
    "    corpus2: Corpus\n",
    "\n",
    "    def compare_scores(self, method='absolute_difference'):\n",
    "        scores = []\n",
    "        for line1, line2 in zip(self.corpus1.lines, self.corpus2.lines):\n",
    "            if method == 'absolute_difference':\n",
    "                score = abs(line1.normalized_score - line2.normalized_score)\n",
    "            elif method == 'euclidean_distance':\n",
    "                score = np.linalg.norm(np.array(line1.scores) - np.array(line2.scores))\n",
    "            elif method == 'cosine_similarity':\n",
    "                score = np.dot(line1.scores, line2.scores) / (np.linalg.norm(line1.scores) * np.linalg.norm(line2.scores))\n",
    "            scores.append(score)\n",
    "        return scores\n",
    "\n",
    "def launch():\n",
    "    def compare_corpora(source_text, target_text, corpus_scoring_method, cross_linguistic_method):\n",
    "        corpus1 = Corpus(text=source_text)\n",
    "        corpus2 = Corpus(text=target_text)\n",
    "\n",
    "        corpus1.calculate_scores()\n",
    "        corpus2.calculate_scores()\n",
    "\n",
    "        comparison = CrossLinguisticComparison(corpus1=corpus1, corpus2=corpus2)\n",
    "        cross_scores = comparison.compare_scores(method=cross_linguistic_method)\n",
    "\n",
    "        output_table = []\n",
    "        for line1, line2, cross_score in zip(corpus1.lines, corpus2.lines, cross_scores):\n",
    "            output_table.append([\n",
    "                f\"{line1.content}\",\n",
    "                \"\",\n",
    "                \"\"\n",
    "            ])\n",
    "            output_table.append([\n",
    "                f\"Scores: {line1.scores}\",\n",
    "                f\"Scores: {line2.scores}\",\n",
    "                \"\"\n",
    "            ])\n",
    "            output_table.append([\n",
    "                f\"Normalized Score: {line1.normalized_score:.2f}\",\n",
    "                f\"Normalized Score: {line2.normalized_score:.2f}\",\n",
    "                cross_score\n",
    "            ])\n",
    "\n",
    "        return output_table\n",
    "\n",
    "    iface = gr.Interface(\n",
    "        fn=compare_corpora,\n",
    "        inputs=[\n",
    "            gr.Textbox(label=\"Source Text\", lines=5, value=\"Source sentence 1\\nSource sentence 2\\nSource sentence 3\\nSource sentence 4\\nSource sentence 5\"),\n",
    "            gr.Textbox(label=\"Target Text\", lines=5, value=\"Target sentence 1\\nTarget sentence 2\\nTarget sentence 3\\nTarget sentence 4\\nTarget sentence 5\"),\n",
    "            gr.Dropdown(label=\"Corpus Scoring Method\", choices=[\"relative_length\", \"bleu\"], value=\"bleu\"),\n",
    "            gr.Dropdown(label=\"Cross-Linguistic Scoring Method\", choices=[\"absolute_difference\", \"euclidean_distance\", \"cosine_similarity\"], value=\"absolute_difference\")\n",
    "        ],\n",
    "        outputs=gr.Dataframe(headers=[\"Source\", \"Target\", \"Cross-Linguistic Score\"]),\n",
    "        title=\"Novel Machine Translation Scoring\",\n",
    "        description=\"Enter the source text and target text to compare their internal consistency and cross-linguistic scores.\",\n",
    "    )\n",
    "\n",
    "    iface.launch()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
